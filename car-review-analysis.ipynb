{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "F1 Score: 0.8571428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Texts: Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta camioneta para mis entregas de negocios y uso personal\n",
      "Bleu score: 0.7671176261207451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride quality, reliability\n",
      "the Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment . the financial arrangement is quite reasonable; the handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down\n"
     ]
    }
   ],
   "source": [
    "# # Import necessary libraries\n",
    "# from transformers import pipeline\n",
    "# import pandas as pd\n",
    "# import evaluate\n",
    "\n",
    "# # Load the dataset\n",
    "# df = pd.read_csv('car_reviews.csv', delimiter=';')\n",
    "# train = df['Review']\n",
    "\n",
    "# # Sentiment Classification\n",
    "# # Define model name\n",
    "# model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# # Load sentiment analysis pipeline\n",
    "# classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "\n",
    "# # Classify sentiments\n",
    "# predicted_labels = [classifier(x) for x in train]\n",
    "\n",
    "# # Extract labels\n",
    "# label = [x[0]['label'] for x in predicted_labels]\n",
    "\n",
    "# # Convert labels to binary predictions\n",
    "# predictions = list(map(lambda x: 1 if x == 'POSITIVE' else 0, label))\n",
    "\n",
    "# # True labels\n",
    "# true_labels = [1 if x == 'POSITIVE' else 0 for x in df['Class']]\n",
    "\n",
    "# # Evaluate accuracy and F1 score\n",
    "# accuracy = evaluate.load(\"accuracy\")\n",
    "# f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# accuracy_result = accuracy.compute(references=true_labels, predictions=predictions)['accuracy']\n",
    "# f1_result = f1.compute(references=true_labels, predictions=predictions)['f1']\n",
    "\n",
    "# print(f\"Accuracy: {accuracy_result}\")\n",
    "# print(f\"F1 Score: {f1_result}\")\n",
    "\n",
    "# # Translation\n",
    "# # Extract the first two sentences from the first review\n",
    "# new = df['Review'][0].split('.')\n",
    "# first_two = new[0:2]\n",
    "# rev_1 = \".\".join(first_two)\n",
    "\n",
    "# # Load translation pipeline\n",
    "# model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "# translator = pipeline('translation', model=model_name, max_length=30)\n",
    "\n",
    "# # Translate text\n",
    "# translations = translator(rev_1)\n",
    "# translated_review = translations[0]['translation_text']\n",
    "\n",
    "# print(f\"Translated Texts: {translated_review}\")\n",
    "\n",
    "# # Load reference translations\n",
    "# with open('reference_translations.txt', 'r') as file:\n",
    "#     references = file.read().split('\\n')\n",
    "\n",
    "# # Evaluate BLEU score\n",
    "# bleu = evaluate.load(\"bleu\")\n",
    "# results = bleu.compute(predictions=[translated_review], references=[references])\n",
    "# bleu_score = results['bleu']\n",
    "\n",
    "# print(f\"Bleu score: {bleu_score}\")\n",
    "\n",
    "# # Question Answering\n",
    "# # Use the second review as context\n",
    "# rev2_context = df['Review'][1]\n",
    "\n",
    "# # Load question-answering pipeline\n",
    "# model_name = \"deepset/minilm-uncased-squad2\"\n",
    "# qa_model = pipeline('question-answering', model=model_name)\n",
    "\n",
    "# # Define question and context\n",
    "# question = \"What did he like about the brand?\"\n",
    "# context = rev2_context\n",
    "\n",
    "# # Get answer\n",
    "# outputs = qa_model(question=question, context=context)\n",
    "# answer = outputs['answer']\n",
    "\n",
    "# print(answer)\n",
    "\n",
    "# # Summarization\n",
    "# # Summarize the last review\n",
    "# rev_last = df['Review'][4]\n",
    "\n",
    "# # Load summarization pipeline\n",
    "# model_name = 't5-small'\n",
    "# summarizer = pipeline(\"summarization\", model=model_name)\n",
    "\n",
    "# # Generate summary\n",
    "# outputs = summarizer(rev_last, min_length=50, max_length=55)\n",
    "# summarized_text = outputs[0]['summary_text']\n",
    "\n",
    "# print(summarized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I am very satisfied with my 2014 Nissan NV SL. I use this van for my business deliveries and personal use. Camping, road trips, etc. We dont have any children so I store most of the seats in my warehouse. I wanted the passenger van for the rear air conditioning. We drove our van from Florida to California for a Cross Country trip in 2014. We averaged about 18 mpg. We drove thru a lot of rain and It was a very comfortable and stable vehicle. The V8 Nissan Titan engine is a 500k mile engine. It has been tested many times by delivery and trucking companies. This is why Nissan gives you a 5 year or 100k mile bumper to bumper warranty. Many people are scared about driving this van because of its size. But with front and rear sonar sensors, large mirrors and the back up camera. It is easy to drive. The front and rear sensors also monitor the front and rear sides of the bumpers making it easier to park close to objects. Our Nissan NV is a Tow Monster. It pulls our 5000 pound travel trailer like its not even there. I have plenty of power to pass a vehicle if needed. The 5.6 liter engine produces 317 hp. I have owned Chevy and Ford vans and there were not very comfortable and had little cockpit room. The Nissan NV is the only vehicle made that has the engine forward like a pick up truck giving the driver plenty of room and comfort in the cockpit area. I dont have any negatives to say about my NV. This is a wide vehicle. The only modification I would like to see from Nissan is for them to add amber side mirror marker lights.BTW. I now own a 2016 Nissan NVP SL. Love it.\n",
      "Actual Sentiment: POSITIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9294)\n",
      "\n",
      "Review: The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\n",
      "Actual Sentiment: NEGATIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.8654)\n",
      "\n",
      "Review: My first foreign car. Love it, I would buy another.\n",
      "Actual Sentiment: POSITIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9995)\n",
      "\n",
      "Review: I've come across numerous reviews praising the Rogue, and I genuinely feel like I might be missing something. It's only been a week since I got the car, and I am genuinely disappointed. I truly wish I could return it. My main concern revolves around what I see as a significant design flaw (which I believe also exists in the Murano, though that wasn't much better and considerably pricier). The rear windshield is just too small. The headrests in the back seat obstruct the sides of the rearview window. This \"Crossover\" feels more like a cheaply made compact car. My other vehicle is a Sonata, and it provides a significantly quieter and smoother ride. I did not anticipate this car to ride so roughly; my 2006 Pathfinder had a smoother ride! I would rate this car a 5 all around.\n",
      "Actual Sentiment: NEGATIVE\n",
      "Predicted Sentiment: NEGATIVE (Confidence: 0.9935)\n",
      "\n",
      "Review: I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth.\n",
      "Actual Sentiment: POSITIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9987)\n",
      "\n",
      "Accuracy: 0.8\n",
      "F1 result: 0.8571428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Your input_length: 365 is bigger than 0.9 * max_length: 27. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model translation:\n",
      "Estoy muy satisfecho con mi 2014 Nissan NV SL. Uso esta furgoneta para mis entregas de negocios y uso personal.\n",
      "Spanish translation references:\n",
      "['Estoy muy satisfecho con mi Nissan NV SL 2014. Utilizo esta camioneta para mis entregas comerciales y uso personal.', 'Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta furgoneta para mis entregas comerciales y uso personal.']\n",
      "0.6022774485691839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\n",
      "Answer:  ride quality, reliability\n",
      "Original text:\n",
      "I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e4cf76c48343fdab458e3d577e5d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Rashid Iqbal\\.cache\\huggingface\\hub\\models--cnicu--t5-small-booksum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:991: UserWarning: Not enough free disk space to download the file. The expected file size is: 242.09 MB. The target location C:\\Users\\Rashid Iqbal\\.cache\\huggingface\\hub\\models--cnicu--t5-small-booksum\\blobs only has 193.81 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9619710704b94e44b7507e1f78d6bb76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:991: UserWarning: Not enough free disk space to download the file. The expected file size is: 242.09 MB. The target location C:\\Users\\Rashid Iqbal\\.cache\\huggingface\\hub\\models--cnicu--t5-small-booksum\\blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35359060a3914a07ab750af96069abd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  82%|########2 | 199M/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model cnicu/t5-small-booksum with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSeq2SeqLM'>, <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>, <class 'transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration'>). See the original errors:\n\nwhile loading with AutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3557, in from_pretrained\n    resolved_archive_file = cached_file(\n                            ^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 402, in cached_file\n    resolved_file = hf_hub_download(\n                    ^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 552, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\nwhile loading with TFAutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2873, in from_pretrained\n    raise EnvironmentError(\nOSError: cnicu/t5-small-booksum does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n\nwhile loading with T5ForConditionalGeneration, an error is thrown:\nTraceback (most recent call last):\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3557, in from_pretrained\n    resolved_archive_file = cached_file(\n                            ^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 402, in cached_file\n    resolved_file = hf_hub_download(\n                    ^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 552, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\nwhile loading with TFT5ForConditionalGeneration, an error is thrown:\nTraceback (most recent call last):\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2873, in from_pretrained\n    raise EnvironmentError(\nOSError: cnicu/t5-small-booksum does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Load summarization pipeline and perform inference\u001b[39;00m\n\u001b[0;32m     98\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnicu/t5-small-booksum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 99\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[0;32m    100\u001b[0m outputs \u001b[38;5;241m=\u001b[39m summarizer(text_to_summarize, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m53\u001b[39m)\n\u001b[0;32m    101\u001b[0m summarized_text \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    894\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 895\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    896\u001b[0m         model,\n\u001b[0;32m    897\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    898\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    899\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    900\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    901\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    902\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    903\u001b[0m     )\n\u001b[0;32m    905\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    906\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\transformers\\pipelines\\base.py:299\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    298\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m         )\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    304\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model cnicu/t5-small-booksum with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSeq2SeqLM'>, <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>, <class 'transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration'>). See the original errors:\n\nwhile loading with AutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3557, in from_pretrained\n    resolved_archive_file = cached_file(\n                            ^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 402, in cached_file\n    resolved_file = hf_hub_download(\n                    ^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 552, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\nwhile loading with TFAutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2873, in from_pretrained\n    raise EnvironmentError(\nOSError: cnicu/t5-small-booksum does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n\nwhile loading with T5ForConditionalGeneration, an error is thrown:\nTraceback (most recent call last):\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3557, in from_pretrained\n    resolved_archive_file = cached_file(\n                            ^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 402, in cached_file\n    resolved_file = hf_hub_download(\n                    ^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py\", line 552, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\nwhile loading with TFT5ForConditionalGeneration, an error is thrown:\nTraceback (most recent call last):\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anaconda\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2873, in from_pretrained\n    raise EnvironmentError(\nOSError: cnicu/t5-small-booksum does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n\n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load the car reviews dataset\n",
    "file_path = \"car_reviews.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=\";\")\n",
    "\n",
    "# Put the car reviews and their associated sentiment labels in two lists\n",
    "reviews = df['Review'].tolist()\n",
    "real_labels = df['Class'].tolist()\n",
    "\n",
    "\n",
    "# Instruction 1: sentiment classification\n",
    "\n",
    "# Load a sentiment analysis LLM into a pipeline\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# Perform inference on the car reviews and display prediction results\n",
    "predicted_labels = classifier(reviews)\n",
    "for review, prediction, label in zip(reviews, predicted_labels, real_labels):\n",
    "    print(f\"Review: {review}\\nActual Sentiment: {label}\\nPredicted Sentiment: {prediction['label']} (Confidence: {prediction['score']:.4f})\\n\")\n",
    "\n",
    "# Load accuracy and F1 score metrics    \n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Map categorical sentiment labels into integer labels\n",
    "references = [1 if label == \"POSITIVE\" else 0 for label in real_labels]\n",
    "predictions = [1 if label['label'] == \"POSITIVE\" else 0 for label in predicted_labels]\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy_result_dict = accuracy.compute(references=references, predictions=predictions)\n",
    "accuracy_result = accuracy_result_dict['accuracy']\n",
    "f1_result_dict = f1.compute(references=references, predictions=predictions)\n",
    "f1_result = f1_result_dict['f1']\n",
    "print(f\"Accuracy: {accuracy_result}\")\n",
    "print(f\"F1 result: {f1_result}\")\n",
    "\n",
    "\n",
    "# Instruction 2: Translation\n",
    "\n",
    "# Load translation LLM into a pipeline and translate car review\n",
    "first_review = reviews[0]\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "translated_review = translator(first_review, max_length=27)[0]['translation_text']\n",
    "print(f\"Model translation:\\n{translated_review}\")\n",
    "\n",
    "# Load reference translations from file\n",
    "with open(\"reference_translations.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "references = [line.strip() for line in lines]\n",
    "print(f\"Spanish translation references:\\n{references}\")\n",
    "\n",
    "# Load and calculate BLEU score metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_score = bleu.compute(predictions=[translated_review], references=[references])\n",
    "print(bleu_score['bleu'])\n",
    "\n",
    "\n",
    "# Instruction 3: extractive QA\n",
    "\n",
    "# Import auto classes (optional: can be solved via pipelines too)\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# Instantiate model and tokenizer\n",
    "model_ckp = \"deepset/minilm-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
    "\n",
    "# Define context and question, and tokenize them\n",
    "context = reviews[1]\n",
    "print(f\"Context:\\n{context}\")\n",
    "question = \"What did he like about the brand?\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference and extract answer from raw outputs\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "\n",
    "# Decode and show answer\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(\"Answer: \", answer)\n",
    "\n",
    "\n",
    "# Instruction 4\n",
    "\n",
    "# Get original text to summarize upon car review\n",
    "text_to_summarize = reviews[-1]\n",
    "print(f\"Original text:\\n{text_to_summarize}\")\n",
    "\n",
    "# Load summarization pipeline and perform inference\n",
    "model_name = \"cnicu/t5-small-booksum\"\n",
    "summarizer = pipeline(\"summarization\", model=model_name)\n",
    "outputs = summarizer(text_to_summarize, max_length=53)\n",
    "summarized_text = outputs[0]['summary_text']\n",
    "print(f\"Summarized text:\\n{summarized_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
